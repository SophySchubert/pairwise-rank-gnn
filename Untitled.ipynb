{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8445d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophia.schubert/miniconda3/envs/experiment/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spektral\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from spektral.data import DisjointLoader\n",
    "from spektral.data.loaders import tf_loader_available\n",
    "import scipy.sparse as sp\n",
    "from spektral.data.utils import (\n",
    "    prepend_none,\n",
    "    sp_matrices_to_sp_tensors,\n",
    "    to_disjoint,\n",
    "    collate_labels_disjoint\n",
    ")\n",
    "import numpy as np\n",
    "from ogb.graphproppred import GraphPropPredDataset\n",
    "from spektral.data import Dataset, Graph\n",
    "from spektral.datasets import TUDataset, QM9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6158171",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 1,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'dataset': 'ogbg-molesol', #JA: QM9, ogbg-molesol, ogbg-molfreesolv, ogbg-mollipo, ZINC| NEIN: aspirin\n",
    "    'train_test_split': 0.8\n",
    "}\n",
    "\n",
    "np.random.seed(config['seed'])\n",
    "tf.random.set_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9cd1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OGBDataset(Dataset):\n",
    "    '''\n",
    "    (spektral) Dataset class wrapper for Open Graph Benchmark datasets.\n",
    "    '''\n",
    "    def __init__(self, name, **kwargs):\n",
    "        self.name = name\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        dataset = GraphPropPredDataset(name=self.name)\n",
    "        graphs = []\n",
    "        for data in dataset:\n",
    "            edge_index = data[0]['edge_index']\n",
    "            edge_feat = data[0]['edge_feat']\n",
    "            node_feat = data[0]['node_feat']\n",
    "            label = data[1]\n",
    "\n",
    "            # Create adjacency matrix\n",
    "            num_nodes = node_feat.shape[0]\n",
    "            adj = np.zeros((num_nodes, num_nodes))\n",
    "            for edge in edge_index.T:\n",
    "                adj[edge[0], edge[1]] = 1\n",
    "\n",
    "            # Create spektral Graph object\n",
    "            graphs.append(Graph(x=node_feat, a=adj, e=edge_feat, y=label))\n",
    "\n",
    "        return graphs\n",
    "\n",
    "def ogb_available_datasets():\n",
    "    #These regression datasets have size % 2 == 0 number of graphs\n",
    "    return ['ogbg-molesol', 'ogbg-molfreesolv', 'ogbg-mollipo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5122967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(name: str):\n",
    "    '''\n",
    "    Loads a dataset from [TUDataset, OGB]\n",
    "    '''\n",
    "    if name == 'QM9':\n",
    "        dataset = QM9(amount=1000)# 1000 and 100000 ok\n",
    "    elif name in TUDataset.available_datasets():\n",
    "        dataset = TUDataset(name)\n",
    "    elif name in ogb_available_datasets():\n",
    "        dataset= OGBDataset(name)\n",
    "    else:\n",
    "        raise ValueError(f'Dataset {name} unknown')\n",
    "\n",
    "    return dataset, dataset.n_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313cc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_data(data, train_test_split, seed):\n",
    "    '''\n",
    "    Split the data into train and test sets\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    idxs = np.random.permutation(len(data))\n",
    "    split = int(train_test_split * len(data))\n",
    "    idx_train, idx_test = np.split(idxs, [split])\n",
    "    train, test = data[idx_train], data[idx_test]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd00116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    seed = config['seed']\n",
    "    train_test_split = config['train_test_split']\n",
    "    name = config['dataset']\n",
    "\n",
    "    # Load data\n",
    "    data, config['n_out'] = _load_data(name)\n",
    "    # Split data\n",
    "    train_data, test_data = _split_data(data, train_test_split, seed)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58deeaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf_signature(signature):\n",
    "    \"\"\"\n",
    "    Converts a Dataset signature to a TensorFlow signature. Extended keys (idx_a, idx_b) for MyDisjointLoader.\n",
    "    :param signature: a Dataset signature.\n",
    "    :return: a TensorFlow signature.\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    keys = [\"x\", \"a\", \"e\", \"i\", \"idx_a\", \"idx_b\"]\n",
    "    for k in keys:\n",
    "        if k in signature:\n",
    "            shape = signature[k][\"shape\"]\n",
    "            dtype = signature[k][\"dtype\"]\n",
    "            spec = signature[k][\"spec\"]\n",
    "            output.append(spec(shape, dtype))\n",
    "    output = tuple(output)\n",
    "    if \"y\" in signature:\n",
    "        shape = signature[\"y\"][\"shape\"]\n",
    "        dtype = signature[\"y\"][\"dtype\"]\n",
    "        spec = signature[\"y\"][\"spec\"]\n",
    "        output = (output, spec(shape, dtype))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b36b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDisjointLoader(DisjointLoader):\n",
    "    \"\"\"\n",
    "    Extension of DisjointLoader class from spektral library. Additionally to data and targets, it also returns ranking pair indices.\n",
    "    A Loader for [disjoint mode](https://graphneural.network/data-modes/#disjoint-mode).\n",
    "\n",
    "    This loader represents a batch of graphs via their disjoint union.\n",
    "\n",
    "    The loader automatically computes a batch index tensor, containing integer\n",
    "    indices that map each node to its corresponding graph in the batch.\n",
    "\n",
    "    The adjacency matrix os returned as a SparseTensor, regardless of the input.\n",
    "\n",
    "    If `node_level=False`, the labels are interpreted as graph-level labels and\n",
    "    are stacked along an additional dimension.\n",
    "    If `node_level=True`, then the labels are stacked vertically.\n",
    "\n",
    "    **Note:** TensorFlow 2.4 or above is required to use this Loader's `load()`\n",
    "    method in a Keras training loop.\n",
    "\n",
    "    **Arguments**\n",
    "\n",
    "    - `dataset`: a graph Dataset;\n",
    "    - `node_level`: bool, if `True` stack the labels vertically for node-level\n",
    "    prediction;\n",
    "    - `batch_size`: size of the mini-batches;\n",
    "    - `epochs`: number of epochs to iterate over the dataset. By default (`None`)\n",
    "    iterates indefinitely;\n",
    "    - `shuffle`: whether to shuffle the data at the start of each epoch.\n",
    "\n",
    "    **Output**\n",
    "\n",
    "    For each batch, returns a tuple `(inputs, labels)`.\n",
    "\n",
    "    `inputs` is a tuple containing:\n",
    "\n",
    "    - `x`: node attributes of shape `[n_nodes, n_node_features]`;\n",
    "    - `a`: adjacency matrices of shape `[n_nodes, n_nodes]`;\n",
    "    - `e`: edge attributes of shape `[n_edges, n_edge_features]`;\n",
    "    - `i`: batch index of shape `[n_nodes]`.\n",
    "\n",
    "    `labels` have shape `[batch, n_labels]` if `node_level=False` or\n",
    "    `[n_nodes, n_labels]` otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, dataset, node_level=False, batch_size=1, epochs=None, shuffle=True, seed=42, radius=4, sampling_ratio=100\n",
    "    ):\n",
    "        self.node_level = node_level\n",
    "        super().__init__(dataset, batch_size=batch_size, epochs=epochs, shuffle=shuffle)\n",
    "        self.seed = seed\n",
    "        self.radius = radius\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "\n",
    "    def collate(self, batch):\n",
    "        idx_a, idx_b, target = self.sample_preference_pairs(batch, seed=self.seed, radius=self.radius, sampling_ratio=self.sampling_ratio)\n",
    "        packed = self.pack(batch)\n",
    "\n",
    "        y = packed.pop(\"y_list\", None)\n",
    "        if y is not None:\n",
    "            y = collate_labels_disjoint(y, node_level=self.node_level)\n",
    "\n",
    "        output = to_disjoint(**packed)\n",
    "        output = sp_matrices_to_sp_tensors(output)\n",
    "\n",
    "        return output + (idx_a, idx_b), target\n",
    "\n",
    "    def load(self):\n",
    "        print(\"load\")\n",
    "        if not tf_loader_available:\n",
    "            raise RuntimeError(\n",
    "                \"Calling DisjointLoader.load() requires \" \"TensorFlow 2.4 or greater.\"\n",
    "            )\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            lambda: self, output_signature=self.tf_signature()\n",
    "        )\n",
    "\n",
    "    def tf_signature(self):\n",
    "        \"\"\"\n",
    "        Adjacency matrix has shape [n_nodes, n_nodes]\n",
    "        Node features have shape [n_nodes, n_node_features]\n",
    "        Edge features have shape [n_edges, n_edge_features]\n",
    "        Targets have shape [*, n_labels]\n",
    "        Pairs have shape [*, 2]\n",
    "        \"\"\"\n",
    "        signature = self.dataset.signature\n",
    "        if \"y\" in signature:\n",
    "            signature[\"y\"][\"shape\"] = prepend_none(signature[\"y\"][\"shape\"]) #(12800,) #(None, 1)\n",
    "        if \"a\" in signature:\n",
    "            signature[\"a\"][\"spec\"] = tf.SparseTensorSpec\n",
    "\n",
    "        signature[\"i\"] = dict()\n",
    "        signature[\"i\"][\"spec\"] = tf.TensorSpec\n",
    "        signature[\"i\"][\"shape\"] = (None,)\n",
    "        signature[\"i\"][\"dtype\"] = tf.as_dtype(tf.int64)\n",
    "\n",
    "        signature[\"idx_a\"] = dict()\n",
    "        signature[\"idx_a\"][\"spec\"] = tf.TensorSpec\n",
    "        signature[\"idx_a\"][\"shape\"] = (None,)\n",
    "        signature[\"idx_a\"][\"dtype\"] = tf.as_dtype(tf.int64)\n",
    "        signature[\"idx_b\"] = dict()\n",
    "        signature[\"idx_b\"][\"spec\"] = tf.TensorSpec\n",
    "        signature[\"idx_b\"][\"shape\"] = (None,)\n",
    "        signature[\"idx_b\"][\"dtype\"] = tf.as_dtype(tf.int64)\n",
    "\n",
    "        return to_tf_signature(signature)\n",
    "\n",
    "    def sample_preference_pairs(self, graphs, radius=4, sampling_ratio=100, seed=42):\n",
    "        seed = self.seed\n",
    "        size = len(graphs)\n",
    "        sample_size = size * radius * sampling_ratio\n",
    "        r = np.arange(size)\n",
    "        S = sp.csr_matrix((r, (r, r)), shape=(size, size))\n",
    "        parts = np.split(S.data, S.indptr[1:-1])\n",
    "        rnd = np.random.default_rng(seed)\n",
    "        for part in parts:\n",
    "            rnd.shuffle(part)\n",
    "        idx_a = np.empty((sample_size,), dtype=np.int64)\n",
    "        idx_b = np.empty((sample_size,), dtype=np.int64)\n",
    "        target = np.ones((sample_size,), dtype=np.float64)\n",
    "        k = 0\n",
    "        for i in range(size):\n",
    "            part = parts[i]\n",
    "            psize = len(part)\n",
    "            for d in range(radius):\n",
    "                ni = (i + d + 1) % size\n",
    "                npart = parts[ni]\n",
    "                npsize = len(npart)\n",
    "                for j in range(sampling_ratio):\n",
    "                    npart_offset = np.roll(npart, d * sampling_ratio + j)\n",
    "                    idx_a[k:k + psize] = part\n",
    "                    if npsize < psize:\n",
    "                        idx_b[k:k + npsize] = npart_offset\n",
    "                        idx_b[k + npsize:k + psize] = npart_offset[:psize - npsize]\n",
    "                    else:\n",
    "                        idx_b[k:k + psize] = npart_offset[:psize]\n",
    "                    if ni < i:\n",
    "                        target[k:k + psize] = 0\n",
    "                    k += psize\n",
    "        return idx_a, idx_b, target.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3feb653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a437a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs, test_graphs = get_data(config)\n",
    "loader_tr = MyDisjointLoader(train_graphs, batch_size=config['batch_size'], epochs=config['epochs'], seed=config['seed'], radius=4, sampling_ratio=100)\n",
    "loader_te = MyDisjointLoader(test_graphs, batch_size=config['batch_size'], epochs=1, seed=config['seed'], radius=1, sampling_ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09074a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "880ba961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458, 9)\n",
      "(458, 458)\n",
      "(940, 3)\n",
      "(458,)\n",
      "(12800,)\n",
      "(12800,)\n"
     ]
    }
   ],
   "source": [
    "for b in loader_tr:\n",
    "  print(b[0][0].shape)\n",
    "  print(b[0][1].shape)\n",
    "  print(b[0][2].shape)\n",
    "  print(b[0][3].shape)\n",
    "  print(b[0][4].shape)\n",
    "  print(b[0][5].shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0e4d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pref_lookup(X, pref_a, pref_b):\n",
    "  X_a = tf.gather(X, pref_a, axis=0)\n",
    "  X_b = tf.gather(X, pref_b, axis=0)\n",
    "  return X_a, X_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b9cd6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from spektral.layers import GlobalSumPool, ECCConv, GraphMasking, GCNConv\n",
    "\n",
    "channels = 256  # Number of channels for GCN layers\n",
    "dropout = 0.5  # Dropout rate for the features\n",
    "\n",
    "def createPairwiseModel():\n",
    "  x = keras.Input(shape=(None, None, 9), dtype=tf.int64)#\n",
    "  a = keras.Input(shape=(None, None, None), dtype=tf.float64, sparse=True)#\n",
    "  e = keras.Input(shape=(None, None, 3),dtype=tf.int64)#\n",
    "  i = keras.Input(shape=(None, None,), dtype=tf.int64)#\n",
    "  pref_a = keras.Input(shape=(None, 12800,), dtype=tf.int64)#\n",
    "  pref_b = keras.Input(shape=(None, 12800,), dtype=tf.int64)#\n",
    "  x_0 = tf.cast(x, tf.float32)\n",
    "  \n",
    "\n",
    "  x_1 = GCNConv(channels, activation=\"relu\")([x_0, a])\n",
    "  x_1 = BatchNormalization()(x_1)\n",
    "  x_1 = Dropout(dropout)(x_1)\n",
    "  x_2 = GCNConv(channels, activation=\"relu\")([x_1, a])\n",
    "  x_2 = BatchNormalization()(x_2)\n",
    "  x_2 = Dropout(dropout)(x_2)\n",
    "  X_utils = GCNConv(1, activation=\"softmax\")([x_2, a])\n",
    "\n",
    "  # X_utils = keras.layers.Dense(1, activation=\"relu\")(x_3)\n",
    "  X_a, X_b = pref_lookup(X_utils, pref_a, pref_b)\n",
    "  out = X_b - X_a\n",
    "\n",
    "  m = keras.Model(\n",
    "    inputs=(x,a,e,i, pref_a, pref_b), outputs=out,\n",
    "    name=\"RankNet\")\n",
    "  m_infer = keras.Model(\n",
    "    inputs=(x,a,e,i), outputs=X_utils,\n",
    "    name=\"RankNet_predictor\")\n",
    "\n",
    "  m.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.001),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy(threshold=.0)])\n",
    "\n",
    "  return m, m_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9f7d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training repeat 1/3...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "in user code:\n\n    /Users/sophia.schubert/miniconda3/envs/experiment/lib/python3.6/site-packages/spektral/layers/convolutional/conv.py:143 _inner_check_dtypes  *\n        return call(inputs, **kwargs)\n    /Users/sophia.schubert/miniconda3/envs/experiment/lib/python3.6/site-packages/spektral/layers/convolutional/gcn_conv.py:101 call  *\n        output = ops.modal_dot(a, output)\n    /Users/sophia.schubert/miniconda3/envs/experiment/lib/python3.6/site-packages/spektral/layers/ops/matmul.py:119 modal_dot  *\n        assert a_ndim in (2, 3), \"Expected a of rank 2 or 3, got {}\".format(a_ndim)\n\n    AssertionError: Expected a of rank 2 or 3, got 4\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5baab72914e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training repeat {i+1}/3...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrn_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreatePairwiseModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mrns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrn_inf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-800c69e8f3a5>\u001b[0m in \u001b[0;36mcreatePairwiseModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/experiment/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 977\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/experiment/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1115\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/experiment/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/experiment/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    886\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/experiment/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: in user code:\n\n    /Users/sophia.schubert/miniconda3/envs/experiment/lib/python3.6/site-packages/spektral/layers/convolutional/conv.py:143 _inner_check_dtypes  *\n        return call(inputs, **kwargs)\n    /Users/sophia.schubert/miniconda3/envs/experiment/lib/python3.6/site-packages/spektral/layers/convolutional/gcn_conv.py:101 call  *\n        output = ops.modal_dot(a, output)\n    /Users/sophia.schubert/miniconda3/envs/experiment/lib/python3.6/site-packages/spektral/layers/ops/matmul.py:119 modal_dot  *\n        assert a_ndim in (2, 3), \"Expected a of rank 2 or 3, got {}\".format(a_ndim)\n\n    AssertionError: Expected a of rank 2 or 3, got 4\n"
     ]
    }
   ],
   "source": [
    "rns = []\n",
    "hs = []\n",
    "for i in range(3):\n",
    "  print(f\"Training repeat {i+1}/3...\")\n",
    "  rn, rn_inf = createPairwiseModel()\n",
    "  h = rn.fit(loader_tr.load(), validation_data=loader_te.load(), epochs=500, verbose=2)\n",
    "  rns.append(rn_inf)\n",
    "  hs.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d4a358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
