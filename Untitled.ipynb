{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8445d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spektral\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ogb.graphproppred import GraphPropPredDataset\n",
    "from spektral.data import Dataset, Graph\n",
    "#from spektral.datasets import TUDataset, QM9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6158171",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'seed': 1,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'dataset': 'ogbg-molesol', #JA: QM9, ogbg-molesol, ogbg-molfreesolv, ogbg-mollipo, ZINC| NEIN: aspirin\n",
    "    'train_test_split': 0.8\n",
    "}\n",
    "\n",
    "np.random.seed(config['seed'])\n",
    "tf.random.set_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9cd1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OGBDataset(Dataset):\n",
    "    '''\n",
    "    (spektral) Dataset class wrapper for Open Graph Benchmark datasets.\n",
    "    '''\n",
    "    def __init__(self, name, **kwargs):\n",
    "        self.name = name\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        dataset = GraphPropPredDataset(name=self.name)\n",
    "        graphs = []\n",
    "        for data in dataset:\n",
    "            edge_index = data[0]['edge_index']\n",
    "            edge_feat = data[0]['edge_feat']\n",
    "            node_feat = data[0]['node_feat']\n",
    "            label = data[1]\n",
    "\n",
    "            # Create adjacency matrix\n",
    "            num_nodes = node_feat.shape[0]\n",
    "            adj = np.zeros((num_nodes, num_nodes))\n",
    "            for edge in edge_index.T:\n",
    "                adj[edge[0], edge[1]] = 1\n",
    "\n",
    "            # Create spektral Graph object\n",
    "            graphs.append(Graph(x=node_feat, a=adj, e=edge_feat, y=label))\n",
    "            \n",
    "        self.size = len(graphs)\n",
    "\n",
    "        return graphs\n",
    "\n",
    "def ogb_available_datasets():\n",
    "    #These regression datasets have size % 2 == 0 number of graphs\n",
    "    return ['ogbg-molesol', 'ogbg-molfreesolv', 'ogbg-mollipo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5122967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(name: str):\n",
    "    '''\n",
    "    Loads a dataset from [TUDataset, OGB]\n",
    "    '''\n",
    "    if name in ogb_available_datasets():\n",
    "        dataset= OGBDataset(name)\n",
    "    else:\n",
    "        raise ValueError(f'Dataset {name} unknown')\n",
    "\n",
    "    return dataset, dataset.n_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313cc5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_data(data, train_test_split, seed):\n",
    "    '''\n",
    "    Split the data into train and test sets\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    idxs = np.random.permutation(len(data))\n",
    "    split = int(train_test_split * len(data))\n",
    "    idx_train, idx_test = np.split(idxs, [split])\n",
    "    train, test = data[idx_train], data[idx_test]\n",
    "    train.size = len(train)\n",
    "    test.size = len(test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd00116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(config):\n",
    "    seed = config['seed']\n",
    "    train_test_split = config['train_test_split']\n",
    "    name = config['dataset']\n",
    "\n",
    "    # Load data\n",
    "    data, config['n_out'] = _load_data(name)\n",
    "    # Split data\n",
    "    train_data, test_data = _split_data(data, train_test_split, seed)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d4a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test = get_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e3a2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 226)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train), len(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1366fa9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OGBDataset(n_graphs=902)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6749e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "us = [g.y for g in dataset_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5581e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sort = np.argsort(us, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd26ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,2,5,165])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a8c716c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(n_nodes=7, n_node_features=9, n_edge_features=3, n_labels=1)\n",
      "Graph(n_nodes=9, n_node_features=9, n_edge_features=3, n_labels=1)\n",
      "Graph(n_nodes=7, n_node_features=9, n_edge_features=3, n_labels=1)\n",
      "Graph(n_nodes=12, n_node_features=9, n_edge_features=3, n_labels=1)\n"
     ]
    }
   ],
   "source": [
    "for d in dataset_test[a]:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be326bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(n_nodes=12, n_node_features=9, n_edge_features=3, n_labels=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test[165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074d941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d26b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a5bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1492fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CustomDataLoader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, batch_size=32, shuffle=True, seed=42, sampling_ratio=20):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "        self.indices = np.arange(len(self.data))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # hier oder iterate_train_random\n",
    "        indices = np.array(self.indices[index*self.batch_size:(index+1)*self.batch_size])\n",
    "        pairs = list(self.iterate_train_random(indices))\n",
    "        #hier iterate_train_random und sind die elements die indizes oder sonst was und was sind dann die utils\n",
    "        batch_data = self.data[indices]#muss ich hier was machen\n",
    "        batch_labels = [g.y for g in batch_data]#oder nur hier\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def iterate_train_random(self, elements):\n",
    "        objects = elements\n",
    "        sort_idx = np.argsort(objects)\n",
    "        olen = objects.size\n",
    "        seed = self.seed + olen\n",
    "        pair_count = (olen * (olen - 1)) // 2\n",
    "        sample_size = min(int(self.sampling_ratio * pair_count), pair_count)\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        sample = rng.choice(pair_count, sample_size, replace=False)\n",
    "        sample_b = (np.sqrt(sample * 2 + 1/4) + 1/2).astype(np.int)\n",
    "        sample_a = sample - (sample_b * (sample_b - 1)) // 2\n",
    "        idx_a = sort_idx[sample_a]\n",
    "        idx_b = sort_idx[sample_b]\n",
    "\n",
    "        return zip(idx_a, idx_b)\n",
    "    \n",
    "    def get_graphs_from_indices(self, idx_pair):\n",
    "        index_a, index_b = idx_pair\n",
    "        print(index_a,index_b )\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "300a316f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OGBDataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-707eabc9ef2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1146\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m         **kwargs)\n\u001b[0m\u001b[0;32m    925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\master\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[0;32m    802\u001b[0m           lambda x: model(x, training=False), args=(concrete_x,))\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_dynamic_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OGBDataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = np.random.rand(1000, 32)  # 1000 samples, 32 features\n",
    "labels = np.random.randint(0, 2, 1000)  # Binary labels\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "sampling_ratio=4\n",
    "data_loader = CustomDataLoader(dataset_test, batch_size=batch_size, sampling_ratio=sampling_ratio)\n",
    "\n",
    "# Example model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.fit(data_loader, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb150387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
